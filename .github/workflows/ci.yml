name: Atlan CI

on:
  push:
    branches: [ "main", "remote-ci-fix-github" ]
  pull_request:
    branches: [ "main" ]

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Start Atlan Server and Run Benchmarks
      run: |
        # Start server in background with nohup to persist
        cd atlan_server
        nohup python -m uvicorn main:app --host 0.0.0.0 --port 8000 > server.log 2>&1 &
        SERVER_PID=$!
        cd ..

        # Wait for server to be ready (poll with retries)
        echo "Waiting for server to start..."
        for i in {1..30}; do
          if curl -s http://localhost:8000/health > /dev/null 2>&1; then
            echo "Server is ready!"
            break
          fi
          echo "Attempt $i/30 - waiting..."
          sleep 2
        done

        # Verify server is up
        curl -f http://localhost:8000/health || { echo "Server failed to start"; cat atlan_server/server.log; exit 1; }

        # Run Core Benchmark
        echo "Running Core Benchmark (110 cases)..."
        python tests/benchmark_universal.py --mode core --output ci_results_core.json

        # Validate Core Results
        python -c "
        import json
        with open('ci_results_core.json') as f:
            data = json.load(f)
        total = data['summary']['total']
        passed = data['summary']['passed']
        failed = data['summary']['failed']
        errors = data['summary']['errors']
        print(f'Core Benchmark: {passed}/{total} passed ({100*passed/total:.1f}%)')
        print(f'Failed: {failed}, Errors: {errors}')
        if passed != total:
            print('FAIL: Core benchmark must be 100%')
            for r in data['results']:
                if r['status'] != 'PASS':
                    print(f\"  - {r['case']['text'][:50]}... Expected: {r['case']['expected_status']}, Got: {r['actual_status']}\")
            exit(1)
        print('PASS: Core benchmark 100%')
        "

        # Run Deep Benchmark
        echo "Running Deep Benchmark (530 cases)..."
        python tests/benchmark_universal.py --mode deep --output ci_results_deep.json

        # Validate Deep Results
        python -c "
        import json
        with open('ci_results_deep.json') as f:
            data = json.load(f)
        total = data['summary']['total']
        passed = data['summary']['passed']
        failed = data['summary']['failed']
        errors = data['summary']['errors']
        tps = data['summary']['tps']
        print(f'Deep Benchmark: {passed}/{total} passed ({100*passed/total:.1f}%)')
        print(f'Failed: {failed}, Errors: {errors}, TPS: {tps:.1f}')
        if passed != total:
            print('FAIL: Deep benchmark must be 100%')
            failures = [r for r in data['results'] if r['status'] != 'PASS'][:10]
            for r in failures:
                print(f\"  - [{r['case']['category']}] {r['case']['text'][:40]}... Expected: {r['case']['expected_status']}, Got: {r['actual_status']}\")
            exit(1)
        print('PASS: Deep benchmark 100%')
        "

        echo "All benchmarks passed!"

    - name: Upload Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: benchmark-results
        path: |
          ci_results_core.json
          ci_results_deep.json
